## âœ… Day 1 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Iâ€™ve successfully completed **Day 1** of the Databricks AI Challenge, and it was an exciting introduction to the **Databricks Lakehouse ecosystem**.

---

## ğŸ“˜ What I Learned

- Why **Databricks** over traditional **Pandas / Hadoop**
- Fundamentals of **Lakehouse architecture**
- Overview of the **Databricks workspace structure**
- Real-world industry use cases:
  - Netflix  
  - Shell  
  - Comcast  

---

## ğŸ› ï¸ Tasks Completed

- Set up **Databricks Community Edition**
- Explored **Workspace**, **Compute**, and **Data Explorer**
- Created my **first Databricks notebook**
- Executed basic **PySpark commands**

---

## âœ… Day 2 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Iâ€™ve successfully completed **Day 2** of the Databricks AI Challenge, focusing on core **Apache Spark fundamentals**.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ Spark Architecture (Driver, Executors, DAG)

- **Driver** manages the Spark application lifecycle, builds the logical execution plan (**DAG**), schedules stages and tasks, and tracks execution progress.
- **Executors** are worker processes that execute tasks, perform computations, and cache data in memory for faster processing.
- The **DAG (Directed Acyclic Graph)** represents the sequence of transformations and is converted into a physical execution plan when an action is triggered.

### ğŸ”¹ DataFrames vs RDDs

- **RDDs** are low-level, immutable distributed collections offering fine-grained control but no automatic query optimization.
- **DataFrames** are high-level, schema-based abstractions with a SQL-like API and built-in optimizations via **Catalyst** and **Tungsten**, making them faster and easier for structured data.

### ğŸ”¹ Lazy Evaluation in Spark

- Transformations are recorded in a DAG but executed only when an action (e.g., `count`, `show`, `collect`) is called.
- This allows Spark to optimize the full pipeline, reducing shuffles and improving overall performance.

### ğŸ”¹ Databricks Notebook Magic Commands (`%sql`, `%python`, `%fs`)

- Enable seamless language switching and file system access within a single notebook, boosting productivity and workflow efficiency.

---

## ğŸ› ï¸ Tasks Completed

- âœ” Uploaded a sample **e-commerce CSV**
- âœ” Loaded data into a **Spark DataFrame**
- âœ” Performed transformations: `select`, `filter`, `groupBy`, `orderBy`
- âœ” Exported processed results

---

Grateful for the initiative and support from **Databricks**, **Codebasics**, and **Indian Data Club** ğŸ™

---

## âœ… Day 3 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Successfully completed **Day 3**, focusing on a deep dive into **PySpark transformations** and how they compare to traditional data processing approaches.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ PySpark vs Pandas

- **PySpark** enables distributed, parallel processing for large-scale datasets.
- **Pandas** is optimized for in-memory, single-machine analysis.

### ğŸ”¹ Joins in PySpark

- Implemented **inner**, **left**, **right**, and **outer joins** to combine large datasets efficiently in a distributed environment.

### ğŸ”¹ Window Functions

- Used window functions for **running totals**, **rankings**, and **partition-based calculations** without collapsing data granularity.

### ğŸ”¹ User-Defined Functions (UDFs)

- Created **UDFs** to apply custom business logic when built-in Spark functions were insufficient.

---

## ğŸ› ï¸ Tasks Completed

-  Loaded the full **e-commerce dataset** into Spark
-  Performed **complex joins** across multiple tables
-  Calculated **running totals** using window functions
-  Created **derived features** for downstream analysis

---

## âœ… Day 4 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 4**, focusing on an introduction to **Delta Lake** and how it brings **reliability and governance** to modern data lakes.

---

### ğŸ”¹ What is Delta Lake?
Delta Lake is an **open-source storage layer** that sits on top of data lakes (typically storing data in **Parquet** files) and adds reliability features such as **ACID transactions, schema enforcement, and time travel**.  
It helps transform a *raw data lake* into a **production-ready, database-like analytics platform** for both batch and streaming workloads.

---

### ğŸ”¹ ACID Transactions
Delta Lake provides **ACID guarantees** (Atomicity, Consistency, Isolation, Durability) for data operations, preventing **partial writes, inconsistent reads, and table corruption**, especially during concurrent writes.

---

### ğŸ”¹ Schema Enforcement
Delta Lake enforces schemas at write time by validating incoming data against a predefined structure, **rejecting invalid or mismatched data** early to keep pipelines stable.

---

### ğŸ”¹ Delta Lake vs Parquet
**Parquet** is an efficient columnar storage format.  
**Delta Lake** builds on Parquet by adding a **transaction log and table semantics**, enabling **updates, deletes, time travel, ACID transactions, and higher data reliability**.

---

### ğŸ› ï¸ Tasks Completed
- âœ” Converted CSV data into **Delta format**  
- âœ” Created **Delta tables** using **SQL and PySpark**  
- âœ” Tested **schema enforcement** to prevent bad data  
- âœ” Handled **duplicate inserts** safely using Delta Lake features


### ğŸ”– Hashtags
`#Databricks` `#AIChallenge` `#ApacheSpark` `#PySpark` `#BigData` `#DataEngineering` `#DatabricksWithIDC`



