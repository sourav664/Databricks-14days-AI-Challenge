## âœ… Day 1 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Iâ€™ve successfully completed **Day 1** of the Databricks AI Challenge, and it was an exciting introduction to the **Databricks Lakehouse ecosystem**.

---

## ğŸ“˜ What I Learned

- Why **Databricks** over traditional **Pandas / Hadoop**
- Fundamentals of **Lakehouse architecture**
- Overview of the **Databricks workspace structure**
- Real-world industry use cases:
  - Netflix  
  - Shell  
  - Comcast  

---

## ğŸ› ï¸ Tasks Completed

- Set up **Databricks Community Edition**
- Explored **Workspace**, **Compute**, and **Data Explorer**
- Created my **first Databricks notebook**
- Executed basic **PySpark commands**

---

## âœ… Day 2 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Iâ€™ve successfully completed **Day 2** of the Databricks AI Challenge, focusing on core **Apache Spark fundamentals**.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ Spark Architecture (Driver, Executors, DAG)

- **Driver** manages the Spark application lifecycle, builds the logical execution plan (**DAG**), schedules stages and tasks, and tracks execution progress.
- **Executors** are worker processes that execute tasks, perform computations, and cache data in memory for faster processing.
- The **DAG (Directed Acyclic Graph)** represents the sequence of transformations and is converted into a physical execution plan when an action is triggered.

### ğŸ”¹ DataFrames vs RDDs

- **RDDs** are low-level, immutable distributed collections offering fine-grained control but no automatic query optimization.
- **DataFrames** are high-level, schema-based abstractions with a SQL-like API and built-in optimizations via **Catalyst** and **Tungsten**, making them faster and easier for structured data.

### ğŸ”¹ Lazy Evaluation in Spark

- Transformations are recorded in a DAG but executed only when an action (e.g., `count`, `show`, `collect`) is called.
- This allows Spark to optimize the full pipeline, reducing shuffles and improving overall performance.

### ğŸ”¹ Databricks Notebook Magic Commands (`%sql`, `%python`, `%fs`)

- Enable seamless language switching and file system access within a single notebook, boosting productivity and workflow efficiency.

---

## ğŸ› ï¸ Tasks Completed

- âœ” Uploaded a sample **e-commerce CSV**
- âœ” Loaded data into a **Spark DataFrame**
- âœ” Performed transformations: `select`, `filter`, `groupBy`, `orderBy`
- âœ” Exported processed results

---

Grateful for the initiative and support from **Databricks**, **Codebasics**, and **Indian Data Club** ğŸ™

---

### ğŸ”– Hashtags
`#Databricks` `#AIChallenge` `#ApacheSpark` `#PySpark` `#BigData` `#DataEngineering` `#DatabricksWithIDC`



