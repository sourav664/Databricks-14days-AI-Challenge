## âœ… Day 1 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Iâ€™ve successfully completed **Day 1** of the Databricks AI Challenge, and it was an exciting introduction to the **Databricks Lakehouse ecosystem**.

---

## ğŸ“˜ What I Learned

- Why **Databricks** over traditional **Pandas / Hadoop**
- Fundamentals of **Lakehouse architecture**
- Overview of the **Databricks workspace structure**
- Real-world industry use cases:
  - Netflix  
  - Shell  
  - Comcast  

---



## âœ… Day 2 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Iâ€™ve successfully completed **Day 2** of the Databricks AI Challenge, focusing on core **Apache Spark fundamentals**.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ Spark Architecture (Driver, Executors, DAG)

- **Driver** manages the Spark application lifecycle, builds the logical execution plan (**DAG**), schedules stages and tasks, and tracks execution progress.
- **Executors** are worker processes that execute tasks, perform computations, and cache data in memory for faster processing.
- The **DAG (Directed Acyclic Graph)** represents the sequence of transformations and is converted into a physical execution plan when an action is triggered.

### ğŸ”¹ DataFrames vs RDDs

- **RDDs** are low-level, immutable distributed collections offering fine-grained control but no automatic query optimization.
- **DataFrames** are high-level, schema-based abstractions with a SQL-like API and built-in optimizations via **Catalyst** and **Tungsten**, making them faster and easier for structured data.

### ğŸ”¹ Lazy Evaluation in Spark

- Transformations are recorded in a DAG but executed only when an action (e.g., `count`, `show`, `collect`) is called.
- This allows Spark to optimize the full pipeline, reducing shuffles and improving overall performance.

### ğŸ”¹ Databricks Notebook Magic Commands (`%sql`, `%python`, `%fs`)

- Enable seamless language switching and file system access within a single notebook, boosting productivity and workflow efficiency.


---

Grateful for the initiative and support from **Databricks**, **Codebasics**, and **Indian Data Club** ğŸ™

---

## âœ… Day 3 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Successfully completed **Day 3**, focusing on a deep dive into **PySpark transformations** and how they compare to traditional data processing approaches.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ PySpark vs Pandas

- **PySpark** enables distributed, parallel processing for large-scale datasets.
- **Pandas** is optimized for in-memory, single-machine analysis.

### ğŸ”¹ Joins in PySpark

- Implemented **inner**, **left**, **right**, and **outer joins** to combine large datasets efficiently in a distributed environment.

### ğŸ”¹ Window Functions

- Used window functions for **running totals**, **rankings**, and **partition-based calculations** without collapsing data granularity.

### ğŸ”¹ User-Defined Functions (UDFs)

- Created **UDFs** to apply custom business logic when built-in Spark functions were insufficient.



---

## âœ… Day 4 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 4**, focusing on an introduction to **Delta Lake** and how it brings **reliability and governance** to modern data lakes.

---

### ğŸ”¹ What is Delta Lake?
Delta Lake is an **open-source storage layer** that sits on top of data lakes (typically storing data in **Parquet** files) and adds reliability features such as **ACID transactions, schema enforcement, and time travel**.  
It helps transform a *raw data lake* into a **production-ready, database-like analytics platform** for both batch and streaming workloads.

---

### ğŸ”¹ ACID Transactions
Delta Lake provides **ACID guarantees** (Atomicity, Consistency, Isolation, Durability) for data operations, preventing **partial writes, inconsistent reads, and table corruption**, especially during concurrent writes.

---

### ğŸ”¹ Schema Enforcement
Delta Lake enforces schemas at write time by validating incoming data against a predefined structure, **rejecting invalid or mismatched data** early to keep pipelines stable.

---

### ğŸ”¹ Delta Lake vs Parquet
**Parquet** is an efficient columnar storage format.  
**Delta Lake** builds on Parquet by adding a **transaction log and table semantics**, enabling **updates, deletes, time travel, ACID transactions, and higher data reliability**.

--- 

## âœ… Day 5 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 5**, diving into **advanced Delta Lake concepts** that are critical for building reliable, high-performance data pipelines.

---

### ğŸ“˜ Key Concepts Learned

#### ğŸ”¹ Time Travel (Version History)
Enables querying **previous versions** of Delta tables, supporting **experiment reproducibility, data debugging, and safe rollbacks** after faulty updates.

---

#### ğŸ”¹ MERGE Operations (Upserts)
Allows **atomic updates and inserts** in a single operation, making it ideal for **incremental data ingestion** and maintaining clean, consistent feature tables.

---

#### ğŸ”¹ OPTIMIZE & ZORDER
- **OPTIMIZE** compacts many small files into larger ones to improve query performance.  
- **ZORDER** colocates related data on disk, significantly speeding up queries that filter on key columns.

---

#### ğŸ”¹ VACUUM (Cleanup)
Removes obsolete files that are no longer required, helping **control storage costs** while keeping Delta tables clean and efficient.

---

## âœ… Day 6 Completed â€“ Databricks 14 Days AI Challenge
Sponsored by Databricks

Completed Day 6, focusing on the Medallion Architecture and how it enables scalable, reliable, and maintainable data pipelines in modern data platforms.

ğŸ“˜ Key Concepts Learned

ğŸ”¹ Medallion Architecture (Bronze â†’ Silver â†’ Gold)
A layered data design where:

Bronze stores raw, immutable ingested data

Silver applies cleaning, enrichment, and deduplication

Gold contains business-ready, aggregated data optimized for analytics

ğŸ”¹ Best Practices for Each Layer

Bronze: Append-only ingestion with minimal transformations

Silver: Data quality checks, schema enforcement, and deduplication

Gold: Optimized for BI, reporting, and analytics use cases

ğŸ”¹ Incremental Processing Patterns
Processes only new or changed data, improving performance and reducing costs. Techniques like MERGE operations and watermarks enable reliable, scalable pipelines.

---

## âœ… Day 7 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 7**, focusing on **Workflows & Job Orchestration in Databricks** and how they help build **reliable, automated data pipelines**.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ Databricks Jobs vs Notebooks

- **Notebooks**
  - Interactive environments
  - Used for development, experimentation, data exploration, and debugging

- **Jobs**
  - Production-ready executions of notebooks or scripts
  - Designed to run automatically using schedules or event triggers

---

### ğŸ”¹ Multi-task Workflows

- A **workflow** is a pipeline composed of multiple dependent tasks
- Allows complex data pipelines to be:
  - Broken into smaller steps
  - More manageable
  - Reusable and modular

---

### ğŸ”¹ Parameters & Scheduling

- **Parameters**
  - Allow the same notebook or job to run with different inputs  
    (e.g., dates, file paths, model versions)

- **Scheduling**
  - Enables automatic execution at fixed intervals:
    - Hourly
    - Daily
    - Weekly

---

### ğŸ”¹ Error Handling

- Detects and logs job or workflow failures
- Prevents silent failures
- Improves overall pipeline reliability

## âœ… Day 8 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 8**, focusing on **Unity Catalog Governance** and how it enables **secure, centralized data governance** across the Databricks Lakehouse.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ Catalog â†’ Schema â†’ Table Hierarchy

- Unity Catalog organizes data using a **three-level structure**:
  - **Catalog** â€“ Top-level container
  - **Schema** â€“ Database
  - **Table** â€“ Actual data storage

---

### ğŸ”¹ Access Control (GRANT / REVOKE)

- Defines **who can read or modify data**
- Uses fine-grained permissions such as:
  - `SELECT`
  - `MODIFY`
- **Example**: Granting analysts read access to a sales table

---

### ğŸ”¹ Data Lineage

- Tracks how data flows across:
  - Tables
  - Views
  - Jobs
- Helps understand:
  - Data origins
  - Downstream impact of changes

---

### ğŸ”¹ Managed vs External Tables

- **Managed Tables**
  - Fully controlled by Databricks
  - Data lifecycle managed automatically

- **External Tables**
  - Data stored in external storage (e.g., S3, ADLS)
  - Databricks manages metadata only
## âœ… Day 9 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 9**, focusing on **SQL Analytics & Dashboards** and how **SQL Warehouses** power fast, interactive analytics in Databricks.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ SQL Warehouses

- Enable scalable, high-performance execution of SQL queries
- Optimized for:
  - BI workloads
  - Analytical queries
  - Interactive reporting

---

### ğŸ”¹ Complex Analytical Queries

- Wrote advanced SQL using:
  - Joins
  - Window functions
  - Aggregations
  - Common Table Expressions (CTEs)
- Used to derive meaningful **business insights**

---

### ğŸ”¹ Dashboard Creation

- Built interactive dashboards to analyze:
  - Revenue trends
  - Conversion funnels
  - Top-performing products

---

### ğŸ”¹ Visualizations & Filters

- Used:
  - Charts
  - Filters
  - Scheduled refreshes
- Enabled dynamic, self-service analytics for stakeholders

## âœ… Day 10 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Completed **Day 10**, focusing on **performance optimization techniques** to improve **query efficiency and scalability** in Databricks.

---

## ğŸ“˜ Key Concepts Learned

### ğŸ”¹ Query Execution Plans

- Learned how Spark SQL converts queries into:
  - Logical plans
  - Physical plans
- Helps identify performance bottlenecks such as:
  - Costly joins
  - Data shuffles
  - Full table scans

---

### ğŸ”¹ Partitioning Strategies

- Applied partitioning on large tables to:
  - Reduce scanned data
  - Improve query performance
- Ensures only **relevant partitions** are read during query execution

---

### ğŸ”¹ OPTIMIZE & ZORDER

- **OPTIMIZE**
  - Compacts many small files into fewer large files
  - Reduces file scan overhead

- **ZORDER**
  - Colocates related data on disk
  - Significantly speeds up filtered queries

---

### ğŸ”¹ Caching Techniques

- Cached frequently accessed tables
- Benefits:
  - Reduces recomputation
  - Improves performance for iterative and repeated workloads


### ğŸ”– Hashtags
`#Databricks` `#AIChallenge` `#ApacheSpark` `#PySpark` `#BigData` `#DataEngineering` `#DatabricksWithIDC`



